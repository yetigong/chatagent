import logging
from langchain.document_loaders import WebBaseLoader
from langchain.document_loaders import TextLoader
from langchain.indexes import VectorstoreIndexCreator
from langchain.chat_models import ChatOpenAI
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

def chat_agent_api(user_input):
  loader = WebBaseLoader("https://lilianweng.github.io/posts/2023-06-23-agent/")
  index = VectorstoreIndexCreator().from_loaders([loader])
  
  paopao_loader = WebBaseLoader("https://tessa71683.lofter.com/")  
  paopao_data = paopao_loader.load()
  
  cxs_loader = TextLoader("./cxs.txt")
  cxs_data = loader.load()
  data = paopao_data + cxs_data
  
  text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 0)
  all_splits = text_splitter.split_documents(data)
  
  vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())
  
  question1 = "help me to write a paragraph to appearance and characteristics of 相柳. "
  question2 = "help me to summary for the storyline between 相柳 and 小夭 in 1000 words in Chinese"
  question3 = "Hi, my name is Bo Pan"
  question4 = "What's my name"
  #docs = vectorstore.similarity_search(question)
  
  logging.basicConfig()
  logging.getLogger('langchain.retrievers.multi_query').setLevel(logging.INFO)
  
  # multi query retriever can help to use LLM to generate queries
  # the original retriever is just the vector store we created above
  retriever_from_llm = MultiQueryRetriever.from_llm(retriever=vectorstore.as_retriever(),
                                                    llm=ChatOpenAI(temperature=0))
  
  # 
  # unique_docs = retriever_from_llm.get_relevant_documents(query=question)
  
  template = """You will be helping with writing novel paragraphs based on the data provided to you, following style of the contents provided. Always say "thanks for asking!" at the end of the answer. 
  {context}
  Write a paragraph of description about this topic based on context above: {question}. It does not have to be using the original text, but it needs to be synthesized with your own language in the original style. The paragraph generated should be in the original language of the text.
  The paragraph generated by you:
  """
  QA_CHAIN_PROMPT = PromptTemplate.from_template(template)
  
  llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0.5)
  # qa_chain = RetrievalQA.from_chain_type(
  #     llm,
  #     retriever=retriever_from_llm,
  #     chain_type_kwargs={"prompt": QA_CHAIN_PROMPT},
  #     return_source_documents=False,
  #     verbose=True
  # )
  
  # result = qa_chain({"query": question2})
  
  # print(result)
  
  
  # try out some agents
  from langchain.agents.agent_toolkits import create_conversational_retrieval_agent
  from langchain.agents.agent_toolkits import create_retriever_tool
  from langchain.agents.openai_functions_agent.base import OpenAIFunctionsAgent
  from langchain.agents.openai_functions_agent.agent_token_buffer_memory import AgentTokenBufferMemory
  
  from langchain.agents import AgentExecutor
  from langchain.schema.messages import SystemMessage
  from langchain.prompts import MessagesPlaceholder
  
  
  tool = create_retriever_tool(
      retriever_from_llm, 
      "search_tonghua_paopao",
      "Searches and returns documents regarding 长相思，相柳，小夭."
  )
  
  tools = [tool]
  
  memory_key = 'bo_chat_history'
  system_message = SystemMessage(
          content=(
              """You will be helping with writing novel paragraphs based on the data looked up by you. Use the tools provided available for look up. It does not have to be using the original text, but it needs to be synthesized with your own language in the original style.
              The paragraph generated by you in Chinese:"""
          )
  )
  agent_prompt_template = OpenAIFunctionsAgent.create_prompt(
          system_message=system_message,
          extra_prompt_messages=[MessagesPlaceholder(variable_name=memory_key)]
      )
  
  agent = OpenAIFunctionsAgent(llm=llm, tools=tools, prompt=agent_prompt_template)
  # create agent from api create_conversational_retrieval_agent
  # agent_executor = create_conversational_retrieval_agent(llm, tools, verbose=True)
  
  # create agent from constructor
  memory = AgentTokenBufferMemory(memory_key=memory_key, llm=llm)
  
  agent_executor = AgentExecutor(agent=agent, tools=tools, memory=memory, verbose=True,
                                     return_intermediate_steps=True)

  response = agent_executor({"input": user_input})
  return response["output"]